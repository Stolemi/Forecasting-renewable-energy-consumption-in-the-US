---
title: "**Determinants and dynamics of renewable energy consumption in the US**"
author: "Gabriele Bertuzzi (981350), Immo Frieden (1900095441), Adriano Pace (1001273), Emiliano Stolz (980203)"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  pdf_document: 
    keep_tex: yes
header-includes:
   \usepackage{float}
 \floatplacement{figure}{H}
font-size: 11pt
bibliography: references.bib 
#csl: https://raw.githubusercontent.com/citation-style-language/styles/master/apa.csl
---
## Abstract
In this paper we forecast the dynamics of renewable energy consumption in the U.S. market. We considered monthly data on energy and fossil fuels from the US Energy Information Administration (EIA), macroeconomic variables from the FRED database (FED St. Louis) and monthly prices of crude oil and natural gas provided by the World Bank. We performed three variable selection methods in order to reduce the dimensionality of the data: hard tresholding, partial least squares and LASSO. Then, we focused on predictions using a recurrent neural network and a factor model. To estimate the factors we relied on two different approaches: PCA and EM algorithm. We all took part in the design of each step of this paper, however Immo Frieden and Emiliano Stolz contributes in particular on the implementation of hard tresholding, PLS and the factor model, while Gabriele Bertuzzi and Adriano Pace on LASSO and recurrent neural network. The RNN forecasts on the test sample are accurate, as well as the results with the one-factor models.


## 1. Introduction
Due to increasing risk of irreperable environmental damages by pollution and overusage of resources, countries around the world try to establish and incentivize the usage of renewable ways of generating energy. Doing so, countries focus not only on shutting down resource intensive ways of energy production but also issue consumption targets for energy produced by renewable sources. Germany for example is about to shut down its last nuclear power plant at the end of this year [@abc] and plans to be independent of energy produced by coal by 2038 [@Federal]. However, the German government also aims to increase the consumption share of renewable energy to 65% by 2030 [@Federal]. The plans of US president Joe Biden are even more ambitious, he wants all electricity to be from renewable sources by 2035 [@Guardian].  
While the production share of renewable energy is heavily influenced by technological constraints and legal considerations, the demand side is much more of an economic problem and requires further investigation as an increase in consumption is supposed to put pressure on the producers and thereby accelerates the phasing out of old technologies. Hence, this study focuses on understanding the determinants as well as the dynamics of the share of renewable energy consumption in total energy consumption. Therefore, the analysis in this paper is twofold.  
Firstly, it analyses the determinants of renewable energy consumption. Thus, variables are selected from a big dataset, which consists of macroeconomic indicators as well as consumption and production of alternative energy forms, using three different methods of variable selection. In line with Bai and Ng (2008), the paper compares the selection based on hard thresholding and LASSO. Additionally, it analyses which variables are selected by the Variables Importance in Projection (VIP) filtering method for partial least squares regressions (PLSR). Secondly, the above mentioned consumption share of renewable energy in the US is predicted and its development is evaluated. Two forecasting methods are compared, dynamic factor models and neural network, and there ability to predict the response variable is discussed.  
Previous economic literature on the determinants of renewable energy consumption is dominated by investigating the relationship between energy consumption and economic growth. While output and employement were found to be key determinants of solar and wind energy [@Sari], a bi-directional causality between both economic growth and renewable energy consumption and between non-renewable and renewable energy consumption was investigated [@Apergis]. Even though there is literature which investigates the influence of other macroeconomic and institutional measures on the consumption of renewable energy, variables are usually already pre-selected and, hence, heavily dependent on prior beliefs (see @Ergun for literature review). Furthermore, by pre-selection of variables there was little need of employing machine learning methods in previous literature and only a few studies use comparable methodology [@Sarkodie]. Thus, this study distinguishes from earlier literature by only imposing limited prior beliefs on what drives consumption of renewable energy and the methodology that arises from this approach.  
While machine learning methodology with respect to forecasting is quite prevalent within energy economics in general [@Ghoddusi], it is more scarcely used with respect to renewable energy consumption in particular. Since most studies suffer from a comparably short time horizon, they either use completely different methodology [@Tsai] or only rely on neural networks [@Azadeh]. Hence, the paper differs from previous literature in scale as of its long time horizon as well as in methodology.   
The rest of the paper is organized as follows: Section 2 describes the data. Then, chapter 3 introduces the methodology for the variable selection and discusses the underlying results, while section 4 does the same for the forecasting part. Chapter 5 concludes. 

## 2. Data description
The final dataset used for the underlying analysis consists of 145 variables describing the economic and the energy situation in the US ($N = 147$) from 1:1973 until 8:2021 ($T = 584$) on a monthly basis. As many of those series are non-stationary, they are transformed mostly using log-differences. In the following the composition and origin of the data will be discussed more detailed.  
Firstly, monthly data on energy and fossil fuels is provided by the US Energy Information Administration (EIA) [@EIA]. The raw file consists of 41 variables including production, consumption and exports time series of energy by source. Nine of those series are discarded as of too many missing values or collinearity. Furthermore, the dependent variable is constructed as the share of the two variables TotalRenewableEnergyConsumption over TotalPrimaryEnergyConsumption and, therefore, those two variables are also dropped from the analysis. Thus, one is left with 31 variables from this source.  
Additionally, the Federal Reserve Bank of St. Louis maintains a large dataset, the FRED database, on macroeconomic performance in the US. 127 variables on topics like output and income, the labor market, money and credit, prices or interest and exchange rate are updated on a monthly basis (see @McCracken for a detailed description of all included variables in the database). Time series are transformed according to the suggested transformation. As for some time series second log-differences are recommended, the first two observation points of all variables cannot be used in the following analysis ($T = 582$). While the transformation yields stationary time series for most of the series, the housing variables have to be excluded as of non-stationarity. Additionally, four series have to be dropped as of too many missings. Thus, 113 variables from this source are kept.  
Moreover, monthly prices of crude oil and natural gas are included to allow for cross-elasticity of renewable energy consumption on price changes of its substitutes. The time series are provided by the Worldbank [@Worldbank]. Data on the price of coal could not be used as of too many missings in the time series.  
All remaining time series are stationary and there are 8 missing values left in the data which have to be replaced as LASSO does not allow for missing observations. For simplicity and as of the comparably small number of missings, those are imputed by the reported value of the previous period. Furthermore, GeothermalEnergyConsumption and NuclearElectricPowerConsumption are found to be collinear and are, therefore, discarded ($N = 145$). An investigation for outliers yields 1912 extreme results out of 84390 elements. Outliers in this case are defined according to the definition of "far out" outliers in earlier literature on boxplots [@Tukey] and are, therefore, either three interquantile ranges ($IQR = Q3 - Q1$) above the third quantile ($Q3$) or three interquantile ranges below the first quantile ($Q1$). These outliers might drive some of the results in the forecasting section. Finally, the data is standardized to fluctuate around zero which is necessary for the dynamic factor model and the PLSR. In the graphs below one can see the dynamics of all transformed, standardized and missings-corrected time series splitted up in three parts for better observability. As shown, the outliers are spread over the full period with the highest spikes in recent years.   

```{r Loading data, include=FALSE}
rm(list = ls())
setwd("/Users/emiliano/Documents/UNIBO/HS21/Machine Learning/Project/CONSEGNATO")#to set the working directory
set.seed(1234)
#Read the data
library(tidyverse)
library(haven)

data<-read_dta("Final_Stationary_data.dta")
data <- data[,-2]
data_1 <- data.frame(data, row.names = 1)

##########################################################################################

#PREPARE THE DATA

#Manage missing value
library(imputeTS)
sum(is.na(data_1))    #8 NAs
colSums(is.na(data_1))
data_1$compapffx <- na_locf(data_1$compapffx, option = "locf", na_remaining = "rev")
data_1$dl_CoalCokeExports <- na_locf(data_1$dl_CoalCokeExports, option = "locf", na_remaining = "rev")
data_1$dl_spperatio <- na_locf(data_1$dl_spperatio, option = "locf", na_remaining = "rev")
data_1$d1_cp3mx <- na_locf(data_1$d1_cp3mx, option = "locf", na_remaining = "rev")
data_1$d1_spdivyield <- na_locf(data_1$d1_spdivyield, option = "locf", na_remaining = "rev")
sum(is.na(data_1))

#Collinearity
library(caret)
findLinearCombos(data_1)
data_1<-as.data.frame(data_1[,-c(39,40)])

#Standardization
data_1<-as.data.frame(scale(data_1, center=TRUE, scale=TRUE))
```
```{r Time series, include=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="Standardized Time Series"}
#Convert and plot time series
ts_data<-ts(data=data_1, start=c(1973,3), end=c(2021, 8), frequency=12, 
             names = colnames(data_1))
ts_data_1<-ts(data=data_1[c(1:191),], start=c(1973,3), end=c(1989, 1), frequency=12, 
              names = colnames(data_1))
ts_data_2<-ts(data=data_1[c(192:386),], start=c(1989,2), end=c(2005, 4), frequency=12, 
              names = colnames(data_1))
ts_data_3<-ts(data=data_1[c(387:582),], start=c(2005,5), end=c(2021, 8), frequency=12, 
              names = colnames(data_1))
```
```{r Time series plot 1, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, out.width="50%", out.height="30%"}
ts_plot_1 <- ts.plot(ts_data_1, gpars=list(col=rainbow(10)))
ts_plot_2 <-ts.plot(ts_data_2, gpars=list(col=rainbow(10)))
```
```{r Time series plot 2, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, out.width="50%", out.height="30%", fig.align="center", fig.cap="Standardized Time Series Plot"}
ts_plot_3 <-ts.plot(ts_data_3, gpars=list(col=rainbow(10)))

```
```{r Data preparation, include=FALSE}
#Outliers
library(forecast)
outliers<-matrix(,)
for(i in 1:ncol(ts_data)){
  outliers<-rbind(outliers, tsoutliers(ts_data[,i]))  #get the outliers for each column

}
outliers[[1]]=NULL
tot_outliers<-sum(sapply(outliers,length))    #Total number of outliers in the dataset: 1912

#Stationarity
library(tseries)
store<-matrix(,nrow=ncol(ts_data), ncol=1)
for (i in 1:ncol(ts_data)){
  store[i]<-adf.test(ts_data[,i])$p.value   #small p-value: reject non stationarity
}
colnames(store)<-c("p value")

#Save corrected dataset
save(data_1, file="Corrected_data.Rda")

```

\newpage
## 3. Variable selection 
### 3.1 Methodology 
Analyzing the determinants of renewable energy consumption with the underlying dataset requires variable selection. In line with earlier literature [@Bai], this study compares the results of three methodological approaches which differ in the reasoning why variables are chosen. 

### 3.1.1 Hard Thresholding 
Hard thresholding tests the relevance of a single predictor for the dependent variable $y_{t}$ [@Bai]. Hence, considering the autoregressive nature of the time series data, it performs one regression of the share of renewable energy consumption on each variable in the dataset $x_{i,t}$ plus its lags $x_{i,t-h}$ and the lags of the dependent variable. While there are always four lags of the dependent variable included in those regressions, the number of lags of the explanatory variables $h$ vary between three different specifications ($h = 1, 3$  or  $6$). For the variable selection, the resulting p-values of $x_{i,t}$ are compared with a Bonferroni corrected 5% significance level (critical value = 0.00036). Thus, only variables with a p-value smaller than the critical value are kept as their predictive power on $y_{t}$ is significant at 5% level.  

```{r Hard Thresholding, include=FALSE}

# METHOD 1: Variable selection following Bai and Ng (2008) "hard thresholding"

# Step 1: regress y on all x and include lags and lags of y 
library(dplyr)

#create lagged variables 
data_lag_1 <- select(mutate(data_1, across(everything() ,lag, .names = "{.col}_lag1")), ends_with("_lag1"))
data_lag_2 <- select(mutate(data_lag_1, across(everything() ,lag, .names = "{.col}_lag2")), ends_with("_lag2"))
data_lag_3 <- select(mutate(data_lag_2, across(everything() ,lag, .names = "{.col}_lag3")), ends_with("_lag3"))
data_lag_4 <- select(mutate(data_lag_3, across(everything() ,lag, .names = "{.col}_lag4")), ends_with("_lag4"))
data_lag_5 <- select(mutate(data_lag_4, across(everything() ,lag, .names = "{.col}_lag5")), ends_with("_lag5"))
data_lag_6 <- select(mutate(data_lag_5, across(everything() ,lag, .names = "{.col}_lag6")), ends_with("_lag6"))
data_1 <- bind_cols(data_1, data_lag_1[,c(1)], data_lag_2[,c(1)], data_lag_3[,c(1)], data_lag_4[,c(1)])
colnames(data_1)<-c(colnames(data_1[,-c(146:149)]), "dl_ShareRenCons_lag1", "dl_ShareRenCons_lag1_lag2", "dl_ShareRenCons_lag1_lag2_lag3", "dl_ShareRenCons_lag1_lag2_lag3_lag4")

#define dataframes and number of variables
data_1 <- as.data.frame(data_1)
data_lag_1 <- as.data.frame(data_lag_1)
data_lag_2 <- as.data.frame(data_lag_2)
data_lag_3 <- as.data.frame(data_lag_3)
data_lag_4 <- as.data.frame(data_lag_4)
data_lag_5 <- as.data.frame(data_lag_5)
data_lag_6 <- as.data.frame(data_lag_6)
n <- 145

#(a) run regression with one lag
varsel_baing_1 <- lapply(2:n, function(x) lm(data_1$dl_ShareRenCons ~ data_1[,x] + data_lag_1[,x] + data_1$dl_ShareRenCons_lag1 + data_1$dl_ShareRenCons_lag1_lag2 + data_1$dl_ShareRenCons_lag1_lag2_lag3 + data_1$dl_ShareRenCons_lag1_lag2_lag3_lag4))

# extract just coefficients with one lag
summaries_1 <- lapply(varsel_baing_1, summary)
p_values_1 <- lapply(summaries_1, function(x) x$coefficients[2, 4])
p_values_1 <- as.data.frame(cbind(p_values_1))


#(b) run regression with three lags
varsel_baing_3 <- lapply(2:n, function(x) lm(data_1$dl_ShareRenCons ~ data_1[,x] + data_lag_1[,x] + data_lag_2[,x] + data_lag_3[,x] + data_1$dl_ShareRenCons_lag1 + data_1$dl_ShareRenCons_lag1_lag2 + data_1$dl_ShareRenCons_lag1_lag2_lag3 + data_1$dl_ShareRenCons_lag1_lag2_lag3_lag4))

# extract just coefficients with three lags
summaries_3 <- lapply(varsel_baing_3, summary)
p_values_3 <- lapply(summaries_3, function(x) x$coefficients[2, 4])
p_values_3 <- as.data.frame(cbind(p_values_3))


#(c) run regression with six lags
varsel_baing_6 <- lapply(2:n, function(x) lm(data_1$dl_ShareRenCons ~ data_1[,x] + data_lag_1[,x] + data_lag_2[,x] + data_lag_3[,x] + data_lag_4[,x] + data_lag_5[,x] + data_lag_6[,x] + data_1$dl_ShareRenCons_lag1 + data_1$dl_ShareRenCons_lag1_lag2 + data_1$dl_ShareRenCons_lag1_lag2_lag3 + data_1$dl_ShareRenCons_lag1_lag2_lag3_lag4))

# extract just coefficients with six lags
summaries_6 <- lapply(varsel_baing_6, summary)
p_values_6 <- lapply(summaries_6, function(x) x$coefficients[2, 4])
p_values_6 <- as.data.frame(cbind(p_values_6))

# Step 2: compare with Bonferroni corrected critical values
#Store variables names
varname <- colnames(data_1[,-c(1, 146, 147, 148, 149)])

#Bonferroni Correction for p-value
crit_value <- 0.05/(ncol(data_1)-5)    #p-value applying Bonferroni Correction: use 5%

#Variable selection for Hard Thresholding - Bai & Ng with 1 lag
p_values_1 <-data.frame(varname, p_values_1)   
var_selected_hard_thresholding_1<-subset(p_values_1, crit_value>p_values_1)    #16 variables selected
colnames(var_selected_hard_thresholding_1)<-c("Varname", "Hard_Thresholding_lag1")

#Variable selection for Hard Thresholding - Bai & Ng with 3 lags
p_values_3 <-data.frame(varname, p_values_3)   
var_selected_hard_thresholding_3<-subset(p_values_3, crit_value>p_values_3)    #14 variables selected
colnames(var_selected_hard_thresholding_3)<-c("Varname", "Hard_Thresholding_lag3")

#Variable selection for Hard Thresholding - Bai & Ng with 6 lags
p_values_6 <-data.frame(varname, p_values_6)   
var_selected_hard_thresholding_6<-subset(p_values_6, crit_value>p_values_6)    #15 variables selected
colnames(var_selected_hard_thresholding_6)<-c("Varname", "Hard_Thresholding_lag6")

comparison <- full_join(var_selected_hard_thresholding_1, var_selected_hard_thresholding_3, by = c("Varname"))
```
```{r Hard Thresholding Result, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
comparison <- full_join(comparison, var_selected_hard_thresholding_6, by = c("Varname"))
comparison
```
As selected variables do barely vary between the different specifications, the relevance of the variables for the share of renewable energy consumption seems to be independent of the timing. Hence, there are no variables that only affect consumption of renewables in the middle term ($h = 6$) but not in the short term ($h = 1$). Focusing on immediate effects, the study, thus, proceeds with the one-lagged results.  
Consumption and production of alternative energy sources including production of renewable energy have a very high predictive power on $y_{t}$. Additionally, various time series from the output and income group of the FRED dataset have a significant impact on the share of renewable energy consumption. Therefore, the result are in line with earlier literature exploring the relationship between output and consumption of renewables and consumption of non-renewables and $y_{t}$ [@Apergis]. Lastly, some of the variability in the dependent variable is explained by the price of natural gas. This is not very surprising as an increase in prices of natural gas is expected to decrease consumption of natural gas and consumers, therefore, would have to substitute. However, it is puzzling that crude oil price as another included commodity price in the analysis is not part of the selected variables.

### 3.1.2 Partial Least Squares (PLS)

Partial Least Squares (PLS) is a *supervised learning* method for dimension reduction, thus dealing with the so-called large *p* small *n* problem.
To form a relationship between the response variable *Y*, which can be both univariate or multivariate, and the set of explanatory variables $X_1,…,X_p$, PLS constructs new explanatory variables called *components* which are linear combinations of the original predictors. 
Differently from Principal Component Regression (PCR), components in PLS are sequentially constructed also considering the response variable of interest. In other words, PLS forms components that capture most of the information in the input variables that is useful for predicting *Y*, while reducing the dimensionality by using fewer components than the number of input variables [@Garthwaite].

The original aim of PLS is to find the relevant linear subspace of the explanatory variables, nevertheless many methods for variable selection using this method have been proposed in order to improve model interpretation. Variable selection methods based on PLS regression can be divided into three main categories: filter-, wrapper-, and embedded methods. This paper focuses on the selection criteria based on *Variable Importance in Projection* (VIP) which belongs to the filter methods class. 
The VIP score of each predictor *j* is computed using the formula below. First, the optimal number of factors has to be determined, which here is done through cross-validation considering the root mean squared error of prediction (RMSEP), as shown in Figure 2, and indicates use of 14 factors.
  $$v_j = \sqrt{p\cdot\frac{\sum_{a=1}^{A}SS_a(w_{aj}/\|w_a\|^2)}{\sum_{a=1}^A(SS_a)}}$$
where $SS_a$ is the sum of squares explained by the *a*th component and $(w_{aj}/\|w_a\|^2)$ is the normalized weight for *j* variable and *a* component, which represents the importance of the *j*th variable [@Mehmood].
```{r PLS-VIP, include=FALSE}
# METHOD 2: Variable selection based on VIP (Variable importance in projection) from PLS
library(pls)
data_1<-data_1[,-c(146:149)]
plsr_oscorespls <- plsr(dl_ShareRenCons~., data=data_1, 
                        na.action=na.omit, scale=TRUE, validation="CV", method="oscorespls")
summary(plsr_oscorespls)    
```
```{r Validation plot, echo=FALSE, out.width="70%", out.height="30%", fig.align='center', fig.cap="Validation Plot"} 
validationplot(plsr_oscorespls, val.type = "RMSEP", main="", xlab="Number of components")  

```
```{r VIP score, include=FALSE}
opt.comp<-14   
p=dim(plsr_oscorespls$coefficients)[1]
X<-data_1[,-1]

# Variable importance in prediction
W <- plsr_oscorespls$loading.weights
Q <- plsr_oscorespls$Yloadings
TT <- plsr_oscorespls$scores
Q2 <- as.numeric(Q) * as.numeric(Q)
Q2TT <- Q2[1:opt.comp] * diag(crossprod(TT))[1:opt.comp]
WW <- W * W/apply(W, 2, function(x) sum(x * x))
VIP_2 <- sqrt(p * apply(sweep(WW[, 1:opt.comp, drop=FALSE],2,Q2TT,"*"), 1, sum)/sum(Q2TT))
```

Selection of more influential variables occurs by comparing each VIP score $v_j$ with a positive threshold value defined by the researcher. Since the average of the squared VIP scores equals 1, generally a ‘greater than one rule’ is used and variables with $v_j>1$ are selected. This is not a statistically justified threshold point and it has been found that a cutoff point between 0.83 and 1.21 might be more appropriate based on certain characteristics of the data such as proportion of relevant predictors and magnitude of correlation [@Chong].

The cutoff point $=1$ is considered. Compared to the hard thresholding method described previously, the number of selected variables based on VIP score is three times larger but all variables selected with the first method are also selected here. Moreover, with very few exceptions, matching variables are the ones with the highest VIP score and from the plot below these are the variables between index 10 and 40 which correspond to the energy related predictors.

```{r VIP score plot, echo=FALSE, out.width="70%", out.height="30%", fig.cap="VIP Scores", fig.align='center'}
plot(VIP_2, ylab="VIP Score", xlab="Index of predictors")  
segments(x0=-5,y0=1,x1=150,y1=1,col="red")

```

```{r Variables selected: VIP, include=FALSE}
#Variables selected:
vip<-data.frame(varname, VIP_2)
var_selected_VIP<-subset(vip, vip[,2]>1)     #48 variables selected 
colnames(var_selected_VIP)<-c("Varname", "VIP_selected")

#HardThresholding - VIP
selected_hard_VIP<-full_join(var_selected_hard_thresholding_1, var_selected_VIP, by=c("Varname"))

```

### 3.1.3 LASSO

The LASSO, firstly introduced by @tibshirani1996regression, is a *supervised learning* technique to implement variable selection. It shrinks some coefficients, associated to some of the the $p$ predictors, to zero. Aside from variable selection, another advantage of this process is to improve the accuracy of the predictions - with respect to the ones of the linear model - through a reduction of the variance which outweighs the increase in the bias.

Theoretically, the LASSO coefficient is defined as:
$$\left(\widehat{\beta}_{0, \lambda}^{L} \ldots \widehat{\beta}_{p, \lambda}^{L}\right)^{\prime}=\underset{b_{0} \ldots b_{p}}{\arg \min } \sum_{i=1}^{n}\left(y_{i}-b_{0}-\sum_{j=1}^{p} x_{i j} b_{j}\right)^{2}+\lambda \sum_{j=1}^{p}\left|b_{j}\right|.$$
The LASSO coefficients minimizes a quantity equal to the residual sum of squares plus a penalty term, called $l_1$ penalty. If the coefficient $\lambda$ (*tuning parameter*) is large enough, some coefficients are shrunk exactly to zero, allowing for variable selection. Therefore, the higher is $\lambda$, the more the penalty matters, the more coefficients are shrunk to zero.

As in every other machine learning method, the estimation of the LASSO is performed considering two key-passages: splitting the data into training and test set, and then consider the training set to estimate the model and tune the parameters (in this case $\lambda$, using cross validation). The first step is trivial, as we consider the first 291 observations as training set (rougly the $50\%$) and the remaining as test set. We fit the LASSO model using the glmnet method in R, specifying alpha=1.

instead, cross validation with time series data can be tricky. We cannot perform the standard k-fold procedure, which is based on picking randomly samples out of the training set and split it again in different folds, one of which will be the test for the validation procedure. It is evident that the random choice of the folds cannot be applied in this context, as there is a temporal dependency across observations that must be preserved. Therefore, we opted for a procedure suitable for time series, based on a *expanding window*: first we train the LASSO on 100 observations and evaluate its prediction on the next 50 of the sequence. Subsequently, we keep iterating this procedure by incorporating the previous test set into a new training set and testing the accuracy on the following observations. The $\lambda$ chosen with cross validation is:

```{r OLS and LASSO, include=FALSE}
# METHOD 3: Variable selection based on LASSO

#EXKURS:OLS
mod <- lm(dl_ShareRenCons ~ ., data = data_1)
mod$coefficients

#Define Training and Test set
train_df <- data_1[1:291,]
test_df <- data_1[292:582,]

library(caret)
ctrlspecs <- trainControl(method = "timeslice",
                          initialWindow = 100,
                          horizon = 50,
                          skip = 20,
                          fixedWindow = FALSE,
                          savePredictions = "all")
grid <- 10^seq(5, -5, length = 500)
model1 <- train(dl_ShareRenCons ~ .,
                data = train_df,
                preProcess = c("center", "scale"),
                method = "glmnet",
                tuneGrid = expand.grid(alpha = 1, lambda = grid),
                trainControl = ctrlspecs)
```
```{r LASSO best lambda, include=TRUE}
model1$bestTune
```
```{r LASSO value, include=FALSE}
round(coef(model1$finalModel, model1$bestTune$lambda),7)
```

```{r LASSO PLOT, echo=FALSE, out.width="70%", out.height="30%", fig.pos="H", fig.cap="RMSE of different $log(lambda), minimum at -5.56$", fig.align='center'}
plot(log(model1$results$lambda),
     model1$results$RMSE,
     xlab = "log(lambda)",
     ylab = "RMSE",
     xlim = c(-10,0))
```
```{r LASSO, include=FALSE}
log(model1$bestTune$lambda)
varImp(model1)
```

We consider this value of the hyperparameter to perform variable selection with LASSO. The selected variable consists of small subset (8 variables) of our initial starting features. To have a glimpse of which variables contributes the most, we consider this variable importance plot, where we consider only the variables associated to non-zero coefficients.

```{r LASSO variable importance, echo=FALSE, out.width="70%", out.height="30%", fig.pos="H", fig.align='center', fig.cap="LASSO Variable importance"} 
ggplot(varImp(model1), top = 8)
```
```{r LASSO VARIABLES, include=FALSE}
#Select variables
coef_LASSO<-as.matrix(coef(model1$finalModel, model1$bestTune$lambda))
coef_LASSO<-data.frame(varname, coef_LASSO[-1])
var_selected_LASSO<-subset(coef_LASSO, abs(coef_LASSO[,2])>0)
colnames(var_selected_LASSO)<-c("Varname", "LASSO_selected")
```

As we have argued before, the LASSO model could be used to perform variable selection by dropping the coefficients that have been shrunk to zero. However, we should be careful to proceed in this way, because, as in any other model, LASSO results comes with an estimation error. If we perform variable selection only based on LASSO, we are neglecting this error.

### 3.2 Compare results 

Now we compare the results of the different variable selection methods. However, for space reasons the table has been omitted. The production of renewable and nuclear energy, as well as the consumption of coal and fossil fuel are selected by all three methods, suggesting that they are indeed important feature in predicting the share of renewable energy consumption in the US market. Between the models, the LASSO selects fewer features, and some are not selected by the other two methods. Moreover, as we argued before, we can't consider LASSO to perform variable selection simply neglecting its estimation error. 

However, considering the other two procedure, there is some consistency: even if PLS selects more variables, all the ones selected with hard thresholding selected are also selected with PLS. To perform predictive analysis using an artificial neural network in the next section, we adopt the subset of features selected with the hard thresholding.

```{r LASSO comparison, include=FALSE}
#Compare the results from the 3 methods. Table has been omitted from the pdf output for space reasons.
final_comparison<-full_join(var_selected_hard_thresholding_1, var_selected_VIP, by = c("Varname"))
final_comparison_LASSO<-full_join(final_comparison, var_selected_LASSO, by=c("Varname"))
final_comparison_LASSO
```

```{r Selected variables dataset, include=FALSE}
#Create the new dataset with only the selected variables (from HardThresholding - 1lag)
data<-data_1    

t_data<-t(data)
varname_complete<-as.data.frame(colnames(data))
colnames(varname_complete)<-c("varname")
data_1<-data.frame(varname_complete, t_data)
selected_hardthresholding<-as.data.frame(var_selected_hard_thresholding_1[,-2])
colnames(selected_hardthresholding)<-c("varname")
final_data<-merge(selected_hardthresholding, data_1, by="varname")
final_data_1<-final_data[,-1]
rownames(final_data_1)<-final_data[,1]     
final_data<-t(final_data)
final_data<-as.numeric(t(final_data_1))         
dim(final_data)<-c(582,16)
colnames(final_data)<-rownames(final_data_1)
final_data<-data.frame(data[,c(1,3)], final_data)
sum(is.na(final_data))

save(final_data, file="Selected_data.Rda")

```


\newpage


## CHAPTER 4: Forecast Analysis

## 4.1 Neural Networks

The first model we consider to predict the share of renewable energy consumption in the U.S. is a *Recurrent Neural Network* (RNN). In general, the family of artificial neural networks is composed by a wide range of models that can be applied in various contexts, from image recognition to documents classification. All neural networks are composed by several layers: an input layer, composed by $p$ contained variables in the dataset, and multiple hidden layers, each of which contains several hidden units (or neurons), that takes as inputs the initial features or the activations of previous layers. These layers build a non-linear function $f(X)$ to predict the response $Y$.

Recurrent Neural Networks are designed to take as input a sequence, therefore they are useful to build predictive models with time series data. Our dataset consist of one time series for each variable, and we want to predict the share of renewables $Y_l = y_t$ using a mini-sequence of vectors of lagged predictors (containing also the target variable) $X_l=\left\{X_{t-1}, X_{t-2}, \ldots, X_{t-L}\right\}$, where $L$ is the number of lags considered. Since $T=582$ and we opted for $L=5$, we created $l=1, \ldots, 577$ $(X_l,Y_l)$ pairs. 

The structure of the RNN is different from the standard feed-forward neural network. The model considers the $577$ mini-sequence of inputs sequentially: each activation vector $A_l$ (composed by $K$ units) inside the hidden layer takes as input the corresponding mini sequence $X_l$ and the previous activation vector $A_{l-1}$. In particular, each unit is is defined as:
$$
A_{l k}=g\left(w_{k 0}+\sum_{j=1}^{p} w_{k j} X_{l j}+\sum_{s=1}^{K} u_{k s} A_{l-1, s}\right)
$$
where $w_{k j}$ for $j=0,\ldots, p$ and $u_{k s}$ for $s=1,\ldots, K$ are the parameters to be estimated and $g(z)$ is a non-linear activation function. In our model, we opted for the hyperbolic tangent:
$$
g(z) = \frac{(e^z - e^{-z})}{(e^z + e^{-z})}.
$$

Each activation vector feeds the output layer and produces a prediction $O_l$:
$$
O_{l}=\beta_{0}+\sum_{k=1}^{K} \beta_{k} A_{l k}
$$
$\beta_k$ for $k=0,\ldots, K$ are the parameters to be estimated.

To run our RNN we installed the Keras and Tensorflow packages, which are essential to perform deep learning. This procedure is *particularly* challenging for Windows users. Thankfully, we were able to install them relying on a [@guide]. We considered the subset of variables selected with the hard thresholding. Our sample is roughly divided $70\%-30\%$ between training and test set. We run a simple AR model with 5 lags to compare its performance with the RNN predictions. In order to run both models, a function which generates five lagged datasets is defined. The R-squared for the AR(5) is:

```{r RNN preparation, include=FALSE}

load("Selected_data.Rda")
xdata <- data.matrix(final_data[, c("dl_ShareRenCons", "awhman", "dl_CoalConsumption","dl_indpro","dl_ipb51222s","dl_ipcongd","dl_ipfinal","dl_ipfpnss","dl_ipncongd","dl_natural_gas_average","dl_natural_gas_us","dl_NaturalGasConsumptionExcludi","dl_NaturalGasImports","dl_NaturalGasPlantLiquidsProduc","dl_NuclearElectricPowerProductio","dl_PetroleumConsumptionExcluding","dl_TotalFossilFuelsConsumption","dl_TotalRenewableEnergyProductio")])
istrain <- vector(mode="logical", length=582)
istrain<-replace(istrain, list = 1:400, TRUE)
xdata <- scale(xdata)
###
lagm <- function(x, k = 1) {
  n <- nrow(x)
  pad <- matrix(NA, k, ncol(x))
  rbind(pad, x[1:(n - k), ])
}
###
arframe <- data.frame(dl_ShareRenCons = xdata[, "dl_ShareRenCons"],
                      L1 = lagm(xdata, 1), L2 = lagm(xdata, 2),
                      L3 = lagm(xdata, 3), L4 = lagm(xdata, 4),
                      L5 = lagm(xdata, 5)
)
###
arframe <- arframe[-(1:5), ]
istrain <- istrain[-(1:5)]
###
###
arfit <- lm(dl_ShareRenCons ~ ., data = arframe[istrain, ])
arpred <- predict(arfit, arframe[!istrain, ])
V0 <- var(arframe[!istrain, "dl_ShareRenCons"])
```
```{r AR r_sq, include=TRUE}
1 - mean((arpred - arframe[!istrain, "dl_ShareRenCons"])^2) / V0
```

To estimate the RNN we must specify the values assumed by some parameters. First, within each activation vector there are 12 hidden units. We are aware that  in the literature there are many rule-of-thumb methods for determining the correct number of neurons, and we relied on the one which suggests to use a number equal to 2/3 the size of the input layer. 

The batch size and the number of epochs are two crucial hyperparameters. The first defines the number of samples (i.e. how many mini-sequence of input vectors$X_l$) to work through before updating the internal model parameters, while the latter defines the number times that the learning algorithm will work through the entire training dataset @brownlee2018difference. We chose both values relying on the trends of the literature. The batch size is equal to 64 (typically under 32 there is a tendency to overfit), whereas the number of epochs is 200. To be sure that we are not overfitting the training data, we plot the history of the mean squared error of the model (labelled as 'loss') for each epoch i.e the learning curve, both for the training and the validation set. We can see that our RNN presents comparable performance on both sets. If, instead, these parallel plots start to depart consistently, it might be a sign that we are overfitting the training data.

```{r RNN, include=FALSE}
n <- nrow(arframe)
xrnn <- data.matrix(arframe[, -1])
xrnn <- array(xrnn, c(n, 18, 5))
xrnn <- xrnn[,, 5:1]
xrnn <- aperm(xrnn, c(1, 3, 2))
dim(xrnn)
###
library(reticulate)
use_condaenv("session")
library(keras)
model <- keras_model_sequential() %>%
  layer_simple_rnn(units = 12,
                   input_shape = list(5, 18),
                   dropout = 0.1, recurrent_dropout = 0.1) %>%
  layer_dense(units = 1)
model %>% compile(optimizer = optimizer_rmsprop(),
                  loss = "mse")
###
history <- model %>% fit(
  xrnn[istrain,, ], arframe[istrain, "dl_ShareRenCons"],
  batch_size = 64, epochs = 200,
  validation_data =
    list(xrnn[!istrain,, ], arframe[!istrain, "dl_ShareRenCons"])
)
```

```{r history, echo=FALSE, message=FALSE, warning=FALSE, out.width="70%", out.height="30%", fig.cap="Learning curve", fig.align='center'}
plot(history)
```

As expected, this model performs considerably better than the AR(5), because it captures complex non-linear relations between our features.

```{r kpred, include=FALSE}
kpred <- predict(model, xrnn[!istrain,, ])
```
```{r RNN r_sq, include=TRUE}
1 - mean((kpred - arframe[!istrain, "dl_ShareRenCons"])^2) / V0
```

This goodness of fit is even more evident when we compare graphically the time series of our predicted share of renewable energy consumption (in red) with the actual one (in black).

```{r plot prep, include=FALSE}
xdata <- as.data.frame(xdata)
test_df <- xdata[401:582,]
x <- test_df$dl_ShareRenCons
y <- kpred
```
```{r final plot, echo=FALSE, out.width="70%", out.height="30%", fig.cap="predicted y vs true y", fig.align='center'}
ts.plot(ts(x), ts(y), col=1:2)
```

\newpage
## 4.2 Factor Models

The second approach implemented for the prediction of the share of renewable energy consumption in the U.S. is *factor analysis*, which allows to work with the entire dataset and it is particularly suited for working with high-dimensional data. In this section an estimation of the factors using two different methods will be provided: (i) Principal Component Analysis; and (ii) Expectation Maximization algorithm. Based on the factors estimated two separate direct forecasts will be performed, more precisely computing two one-step ahead forecasts of the response variable, and the results of both the estimated factors and the estimated predictions will be compared.

Factor analysis is a technique of *unsupervised learning* that in the last few decades, thanks to the increasing availability of large datasets, has been more and more applied in the field of macroeconometrics. Factor models, and in particular Dynamic Factor Model (DFM), are a dimension reduction technique that have been vastly applied in forecasting exercises. The underlying idea is that co-movement of many observed series can be summarized by few unobserved factors, therefore simplifying the high-dimensional problem [@Banbura1]. Forecasting can be then be carried out as a two-step process: first, estimate the factors from the observed predictors; second, estimate by a linear regression the relationship between the variable of interest and the few selected factors [@StockWatson].

This study focuses on a dynamic factor model, where the dynamics of each series is split in two orthogonal components: the first captures the co-movements and is driven by few common factors and the second is treated as an idiosyncratic residual representing individual features [@Banbura2]. Moreover, working with a large dataset (i.e. large $n$), the more realistic approximate DFM can be considered, which allows the idiosyncratic component to be weakly correlated with all other idiosyncratic components.
The approximate DFM in state-space form is therefore written as follow: 
\begin{align}
     x_{it}&=\boldsymbol{\lambda}’_i\boldsymbol{F}_t+\xi_{it} \quad\quad i = 1,…,n,\quad t=1,…,T\\
     \boldsymbol{F}_t&=\boldsymbol{AF}_{t-1}+\boldsymbol{v}_t, \quad\quad   t=1,...,T
\end{align}
where $\boldsymbol{\lambda}_i’\boldsymbol{F}_t$ is the *common* component and $\xi_{it}$ the *idiosyncratic* component. We assume the vector of factors $\boldsymbol{F_t}$ to follow a VAR(1) process with $\{\boldsymbol{v}_t\}$ being a zero-mean white noise process.

### Estimation of the factors

Significant dimension reduction occurs when the number of selected factors is much smaller than the number of original predictors, i.e.  $r\ll n$. Determination of the number of $r$ static factors to estimate in the model can be achieved by a combination of a priori knowledge, visual inspection of a scree plot and the use of information criteria. 
From the cumulative variance of the first 8 and 14 factors it can be observed that already with only 8 almost 50\% of the variability of $\boldsymbol{X}$ is explained. Moreover, the scree plot below shows that after 13 factors the additional explanatory value of an extra factor becomes very small.


```{r Number of factors, include = FALSE}
rm(list = ls())
library(tidyverse)
library(haven)
load("Corrected_data.Rda")

#Determine number of factors
#(i) PCA: 
results<-prcomp(data_1, scale=T)
var_explained = results$sdev^2 / sum(results$sdev^2)
```
```{r Variance explained, include=TRUE}
sum(var_explained[c(1:8)])  
sum(var_explained[c(1:14)])
```
```{r Variance explained 1, include=FALSE}
var_10<-round(sum(var_explained[c(1:10)]), 3)*100 
var_1<-round(sum(var_explained[1]), 3)*100

```
```{r Scree plot, echo=FALSE, out.width="70%", out.height="30%", fig.align="center", fig.cap="Scree plot"}
qplot(c(1:145), var_explained,  ylab="Proportion of Variance Explained", xlab="Number of factors") +
  geom_line() + geom_vline(xintercept = 13, color = "red")
```

Furthermore, the information criteria proposed by @Bai is considered, which is expressed as $IC(r)=\ln V_r(\hat{\Lambda},\hat{F})+r g(N,T)$, where $V_r(\hat{\Lambda},\hat{F})$ is the sum of squared residuals when $r$ factors are estimated using PCA and $g(N,T)$ is a penalty factor. This criteria allows to consistently estimate the number of factors from the observed data in approximate factor models and in this case indicates 10 as the optimal number, which will be considered for the estimation that follows. 
```{r IC, echo=FALSE, message=FALSE, fig.align="center", out.width="70%", out.height="30%", fig.cap="IC plot"}
#(ii) Bai & Ng (2002)
library(nowcasting)
ICF1<-ICfactors(data_1, rmax=16, type=2)   
```


Once the number of factors is determined we can proceed with the estimation of the factors. For the approximate DFM in state-space form, in the case of large $n$, there are two main classes of consistent estimators, which will be implemented hereafter: 
\begin{enumerate}
	\item{Principal Component Analysis (PCA);}
	
	\item{techniques based on Kalman Filter ($KF$) and Expectation Maximization ($EM$) algorithm}
\end{enumerate}

#### Principal Component Analysis

PCA is a non-parametric technique which is simple to implement, does not need any distributional assumption and give consistent estimates of the factors in an approximate factor model for large $n$ [@StockWatson]. Factors are build as linear combinations of the observed variables in such a way that the variance of the idiosyncratic component is minimum and the variance of the common component is maximum. 
The first 10 factors are estimated through PCA, which account for `r var_10`\% of the total variance. In Figure 10 the first 4 factors are plotted and show some extreme values which could be caused by the presence of outliers in the data.

```{r PCA, include=FALSE}
library(psych)
pca<-principal(data_1, nfactors=10, rotate="none", missing = FALSE, cor = "cor")
variance_X <- cbind(pca$communality, pca$uniquenesses)
colnames(variance_X)<-c("Communality", "Uniqueness")    
```
```{r Factor Plot, echo=FALSE, fig.align="center", out.width="70%", out.height="40%", fig.cap="PCA Factors"}
pca_scores<-ts(data=pca$scores, start=c(1973,3), end=c(2021,8), frequency=12, 
               names = colnames(pca$scores))
plot(pca_scores[,1], type="l", ylab="PCA Factors")
lines(pca_scores[,2], col="red")
lines(pca_scores[,3], col="blue")
lines(pca_scores[,4], col="green")
```

Once factors are estimated, the one-step ahead direct forecast of the response variable can be computed. As a benchmark estimation a prediction considering only the first factor is conducted. In direct forecast the dependent variable $y_{t+1}$ is projected onto the factor at time $t$. The results are showed in Figure 11, where it can be observe that the predicted response (red) under-estimate the observed one (black) throughout the series and the variance of the prediction is much smaller. The limited prediction accuracy of the one-factor model is in part determined by the fact that the first factor accounts only for `r var_1`\% of the variability of the data, while the remaining variance is explained by individual features of the series. 

```{r PCA Forecast 1, include=FALSE}
h<-1   #1-step ahead

#1. Compute factors on whole time series and drop the last h-observations
j<-582-h   #drop the NAs and the last h-observations
factors_PCA<-pca$scores[c(1:j), 1]

#2. Extract Y series and drop the first h-observations 
data<-data.matrix(read_dta("Final_Stationary_data.dta")[,3]) 
k<-h+1
y_PCA <- data[c(k:582),1]

#3. Compute beta: regress Y on factors 
```

```{r Beta coefficient, echo=FALSE}
beta_PCA<-(t(factors_PCA)%*%factors_PCA)^(-1)%*%t(factors_PCA)%*%y_PCA 
colnames(beta_PCA)<-c("Beta Coefficient PCA")
beta_PCA
```

```{r PCA Forecast 2, include=FALSE}
#4. Predict Y on factors
y_pred_PCA <- factors_PCA%*%beta_PCA
bbb<-matrix(, nrow=h, ncol=1)
y_pred_1_PCA<-rbind(bbb, y_pred_PCA)
```

```{r Observed-Predicted y, echo=FALSE, warning=FALSE}
y_ypred_PCA<-cbind(y_PCA, y_pred_1_PCA)
colnames(y_ypred_PCA)<-c("Y observed", "Y predicted")
tail(y_ypred_PCA, 8)
```

```{r Observed-Predicted y plot, echo=FALSE, fig.align="center", out.width="70%", out.height="40%", fig.cap="PCA Prediction"}
#5. Plot time series: predicted and observed y
y_ts_PCA<-ts(data=data[,1], start=c(1973,3), end=c(2021,8), frequency=12, 
             names = "Y")
y_pred_1_PCA_ts<-ts(data=y_pred_1_PCA, start=c(1973,3), end=c(2021,8), frequency=12, 
                    names = "Y_pred")
plot(y_ts_PCA, type="l", ylab="y unit")     
lines(y_pred_1_PCA_ts, col="red")

#6. RMSE
rmse_PCA <- sqrt(mean((data[c(k:582),1] - y_pred_1_PCA[c(k:582),])^2))
```

One of the main limitations of PCA estimation of the common component is that it does not take into account the dynamics of the factors themselves that are described in equation (2). Moreover, in the estimation of factors and loadings only the sample covariance is considered and therefore possible cross-sectional correlation of the idiosyncratic terms is not being accounted for. 
For this reason other techniques based on the computation and maximization of the likelihood in general are able to deliver more efficient estimates. In particular, in the following part, the Expectation Maximization algorithm is implemented. 

#### Expectation Maximization algorithm 

The most efficient estimator in a parametric model such as the one we defined above is the Maximum Likelihood estimator, which requires the assumption of Gaussianity of the whole model for $\boldsymbol{x}_t$. 
The EM estimator is fully parametric and is computed as if the idiosyncratic components were serially and cross-sectionally uncorrelated. Nevertheless, Barigozzi & Luciani [-@Barigozzi] showed that the loss in efficiency in the estimation of the factor, due to this potential misspecification of the idiosyncratic covariance matrix, has a negligible impact on the estimates of the common components as $n$ and $T$ increase. The EM algorithm is an iterative procedure which can be employed even when $n$ is large and is based on two sequential steps: an expectation step (E-step) and a maximization step (M-step). The EM estimator converges up to an approximation error, to the maximum likelihood estimator of the parameters of the model. Unlike PCA, the EM procedure considers also the dynamics of the factors in the estimation, delivering in general a more accurate result.  

The factors are estimated considering the common shocks $q$ equal the static factors and the EM algorithm is initialized with the PCA-VAR estimator of the parameters. Estimation of a one-factor model is performed, which, by assuming $q=r=1$, implies that factors follow a AR(1) process and that there are no dynamics in the loadings. From the plot of the single factor in Figure 12 (left) it can be observed that also in this case there are some extreme values throughout the period. As in the PCA estimation, a one-step ahead direct forecast is conducted and again the resulting prediction of the response variable (in red) is under-estimating the observed one throughout the series and shows much lower variance. Same consideration can be made: the single first factor capture the trend of the response variable but the majority of its variability is due to the idiosyncratic component.

```{r EM algorithm, include=FALSE}
load("Corrected_data.Rda")
data<-data_1
aaa<-matrix(, nrow=12, ncol=ncol(data))
data<-rbind(as.matrix(data), aaa)

library(xts)
time<-as.Date(as.matrix(rownames(data_1)), format="%Y-%m-%d") 

#Convert data in time series
ts_data_EM<-ts(data=data, start=c(1973,3), end=c(2022,8), frequency=12, 
            names = colnames(data))
is.ts(ts_data_EM)

######################################
#EXPECTATION MAXIMIZATION algorithm

#1 factors with 2 blocks, thus 2 factors in total
#OSS: in EM is assumed q=r, where r is the number of factor in each block

#Define blocks: estimate factor for subgroups of variables
block<-matrix(1, nrow=ncol(ts_data_EM), ncol=2)   #both factors are computed on all variables


parameters<-list("data"=ts_data_EM, "time"=time, 
                 "blocks"=list("block"=block, "blocks name"=c("Global_1", "Global_2")), 
                 "frequency"=list("frequency"=c(rep(12, ncol(ts_data_EM)))))

block<-parameters$blocks$block
frequency<-parameters$frequency$frequency

nowEM<-nowcast(dl_ShareRenCons~.,ts_data_EM, r=1, q=1, p=1, method="EM",    
               blocks=block, frequency=frequency)   #converge after 15 iterations

```

```{r One factor plot, echo=FALSE, fig.align="center", out.width="70%", out.height="40%", fig.cap="EM Factor", include=FALSE, eval=FALSE}
plot(nowEM$factors$dynamic_factors[,1], type="l", ylab="EM Factor")
```

```{r EM Forecast 1, include=FALSE}

######################################
#H-step ahead forecast
#OSS: try with first 1 factors

h<-1   #1-step ahead

#1. Compute factors on whole time series and drop the last h-observations
j<-594-12-h   #drop the NAs and the last h-observations
factors_EM<-nowEM$factors$dynamic_factors[c(1:j), 1] 

#plot(factors_EM, type="l") #1st factor
#plot(factors_EM[,2], type="l") #1st factor


#2. Extract Y series and drop the first h-observations 
data<-data.matrix(read_dta("Final_Stationary_data.dta")[,3]) 
k<-h+1
y_EM <- data[c(k:582), 1]
```

```{r Beta coefficient EM, echo=FALSE, warning=FALSE}
#3. Compute beta: regress Y on factors 

beta_EM = (t(factors_EM)%*%factors_EM)^(-1)%*%t(factors_EM)%*%y_EM    
colnames(beta_EM)<-c("Beta Coefficient EM")
beta_EM
```

```{r EM Forecast 2, include=FALSE}
#4. Predict Y on factors
y_pred_EM <- factors_EM%*%beta_EM
bbb<-matrix(, nrow=h, ncol=1)
y_pred_1_EM<-rbind(bbb, y_pred_EM)
```
```{r Observed-predicted y EM, echo=FALSE, warning=FALSE}
y_ypred_EM<-cbind(y_EM, y_pred_1_EM)
colnames(y_ypred_EM)<-c("Y observed", "Y predicted")
tail(y_ypred_EM, 8)
```

```{r Observed-predicted y EM plot, echo=FALSE, out.width="50%", out.height="35%", fig.show='hold', fig.cap="EM Factors and Prediction"}
#5. Plot time series: predicted and observed y
y_ts_EM<-ts(data=data[,1], start=c(1973,3), end=c(2021,8), frequency=12, 
            names = "Y")
y_pred_1_EM_ts<-ts(data=y_pred_1_EM, start=c(1973,3), end=c(2021,8), frequency=12, 
                   names = "Y_pred")

plot(nowEM$factors$dynamic_factors[,1], type="l", ylab="EM Factor")    #FACTOR PLOT

plot(y_ts_EM, type="l", ylab="y unit")     
lines(y_pred_1_EM_ts, col="red")



#6. RMSE
rmse_EM <- sqrt(mean((data[c(k:582)] - y_pred_1_EM[c(k:582),])^2))
```

Factor estimation by PCA and EM algorithm delivers similar results regarding the factors estimates, this is mainly due to the fact that both methods do not consider the possible cross-sectional correlation of the idiosyncratic component and the EM algorithm is initialized with the PCA estimation of the model parameters. The main difference between the two methods is that the EM algorithm is able to consider the dynamics of the factors for parameter estimation, which in our one-factor model are described by a AR(1) process. 
In the one-factor model case, the magnitude of the coefficients and the variance of the first factor estimated by PCA are much smaller compared to the one from the EM algorithm (Figure 13 (left)). The prediction accuracy of both models is in any case almost the same, as the computed Root Mean Squared Error (RMSE) and Figure 13 (right) demonstrate. 
A direct forecast considering more than one factor was also performed but the prediction results were completely imprecise and worsened with the number of factor increasing. One possible reason for this is the presence of outliers in the data which can distort the results of PCA estimation, being based on the sample covariance matrix. Moreover, the accuracy of the one-factor model results might indicate that this model specification is adequate in predicting our response variable and that the unexplained variability, being driven by outliers, is idiosyncratic.

```{r PCA-EM factor comparison, echo=FALSE, out.width="50%", out.height="35%", fig.show='hold', fig.cap="PCA - EM Comparison"}
rmse<-cbind(rmse_PCA, rmse_EM)
colnames(rmse)<-c("RMSE PCA", "RMSE EM")
rmse

#Compare factor from PCA and EM 
factors_PCA_EM<-cbind(factors_EM, factors_PCA)
colnames(factors_PCA_EM)<-c("1 factor EM", "1 factor PCA")
plot(factors_PCA_EM[,1], type="l", ylab="1 Factor")
lines(factors_PCA_EM[,2], col="red")
var_F1PCA<-var(factors_PCA)
var_F1EM<-(factors_EM)

#Compare predicted y 
ypred_PCA_EM<-cbind(y_pred_1_EM_ts, y_pred_1_PCA_ts)
colnames(ypred_PCA_EM)<-c("Ypred EM", "Ypred PCA")
plot(ypred_PCA_EM[,1], type="l", ylab="Y Predicted")
lines(ypred_PCA_EM[,2], col="red")
```


## CHAPTER 5: Conclusions

The aim of this paper was to develop machine learning algorithms to forecast the dynamics of renewable energy consumption in the US. First, we performed variable selection recurring hard thresholding, PLS and LASSO. Among these models, the LASSO selects fewer features, whereas PLS the most (three times the variables selected with hard thresholding). There is consistency between PLS and hard thresholding, as the latter selects variables that are selected also by the former. Then, we estimated an AR(5) model and trained a RNN algorithm considering the subset of predictors selected by hard thresholding. The RNN outperforms remarkably the AR(5) model, as the r-squared roughly doubles. Then, we focused on the factor model, estimating the factors with PCA and EM algorithm. Even though slightly different, the predictions with the one-factor model are accurate, whereas when we consider two factors the results are less precise, probably because of the presence of outliers in the data.

\newpage

## Bibliography

<!-- the line below puts the bibliography right after  -->
<div id="refs"></div>    

